{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8922fe3-0316-49f7-8001-1b4c60d83591",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b5ee46-2c54-475b-b11a-fa1c659e74cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#import Company_Tweet.csv\n",
    "Company_Tweet_df = pd.read_csv(Path(\"Company_Tweet.csv\"))\n",
    "Company_Tweet_df = Company_Tweet_df.set_index('tweet_id')\n",
    "Company_Tweet_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3911c1a2-0215-45ef-acc9-e89b50b0dd60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#import Tweet.csv\n",
    "tweet_df = pd.read_csv(Path(\"Tweet.csv\"))\n",
    "tweet_df = tweet_df.set_index('tweet_id')\n",
    "tweet_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585b9c1e-98d1-4f72-b431-aedf9d21f800",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#merge to df into one\n",
    "Company_tweet_result = tweet_df.merge(Company_Tweet_df, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45197158-2f17-458e-9adc-b2b6f2b28cc8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#select relavent ticker and column.\n",
    "selected_df = Company_tweet_result[Company_tweet_result['ticker_symbol'].isin(['AAPL', 'TSLA', 'TWTR'])]\n",
    "selected_df = selected_df[['ticker_symbol', 'post_date','body']]\n",
    "selected_df.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd248b52-e5c8-479f-bb12-957d8535d408",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#convert date into standard format\n",
    "selected_df['post_date'] = pd.to_datetime(selected_df['post_date'], unit='s')\n",
    "selected_df.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead75e1d-986d-417f-a1da-a5930755b416",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use boolean indexing to filter the rows\n",
    "start_date = '2019-01-01 00:00:00'\n",
    "end_date = '2022-12-31 23:59:59'\n",
    "\n",
    "stock_tweet_df = selected_df[(selected_df['post_date'] >= start_date) & (selected_df['post_date'] <= end_date)]\n",
    "\n",
    "stock_tweet_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db10b88-d79a-48c7-ae5e-4c39f54e7243",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f208634-8f03-415f-9f75-078e043485f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Tokenize the body text\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Sample text\n",
    "stock_tweet_df['tokenized_body'] = stock_tweet_df['body'].apply(word_tokenize)\n",
    "\n",
    "# Tokenize\n",
    "print(stock_tweet_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7747a7-6e47-4612-9426-ef85f2dc4470",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    return [word for word in tokens if word.lower() not in stopwords.words('english')]\n",
    "\n",
    "stock_tweet_df['tokenized_body_no_stopwords'] = stock_tweet_df['tokenized_body'].apply(remove_stopwords)\n",
    "\n",
    "print(stock_tweet_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15423174-659b-4616-88ee-4a8bf06227d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import aapl stock price and filter the date\n",
    "aapl_csv = Path(\"AAPL.csv\")\n",
    "aapl_df = pd.read_csv(aapl_csv, index_col=\"Date\", parse_dates=True)\n",
    "aapl_df.sort_index()\n",
    "aapl_df['Ticker'] = 'AAPL'\n",
    "aapl_df = aapl_df[['Ticker', 'Close']]\n",
    "aapl_df.index = pd.to_datetime(aapl_df.index)\n",
    "start_date = '2019-01-01 00:00:00'\n",
    "end_date = '2022-12-31 23:59:59'\n",
    "aapl_df = aapl_df[(aapl_df.index >= start_date) & (aapl_df.index <= end_date)]\n",
    "aapl_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15f12ec-f953-4af6-b14d-ecf34eaa1335",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import tsla stock price and filter the date\n",
    "tsla_csv = Path(\"TSLA.csv\")\n",
    "tsla_df = pd.read_csv(tsla_csv, index_col=\"Date\", parse_dates=True)\n",
    "tsla_df.sort_index()\n",
    "tsla_df['Ticker'] = 'TSLA'\n",
    "tsla_df = tsla_df[['Ticker', 'Close']]\n",
    "tsla_df.index = pd.to_datetime(tsla_df.index)\n",
    "start_date = '2019-01-01 00:00:00'\n",
    "end_date = '2022-12-31 23:59:59'\n",
    "tsla_df = tsla_df[(tsla_df.index >= start_date) & (tsla_df.index <= end_date)]\n",
    "tsla_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d19e7b4b-bcec-4ece-9f7f-0d1944d23814",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#import twer stock price and filter the date\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m twer_csv \u001b[38;5;241m=\u001b[39m \u001b[43mPath\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTWER.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m twer_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(twer_csv, index_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m\"\u001b[39m, parse_dates\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      4\u001b[0m twer_df\u001b[38;5;241m.\u001b[39msort_index()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Path' is not defined"
     ]
    }
   ],
   "source": [
    "#import twer stock price and filter the date\n",
    "twer_csv = Path(\"TWER.csv\")\n",
    "twer_df = pd.read_csv(twer_csv, index_col=\"Date\", parse_dates=True)\n",
    "twer_df.sort_index()\n",
    "twer_df['Ticker'] = 'TWER'\n",
    "twer_df = tsla_df[['Ticker', 'Close']]\n",
    "twer_df.index = pd.to_datetime(twer_df.index)\n",
    "start_date = '2019-01-01 00:00:00'\n",
    "end_date = '2022-12-31 23:59:59'\n",
    "twer_df = twer_df[(twer_df.index >= start_date) & (twer_df.index <= end_date)]\n",
    "twer_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d5a057-c5c8-4c30-a27a-258cecaac607",
   "metadata": {},
   "outputs": [],
   "source": [
    "ethereum_tweets = pd.read_csv(\n",
    "    Path(\"Ethereum_tweets.csv\",\n",
    "    index_col='date', \n",
    "    parse_dates=True, \n",
    "    infer_datetime_format=True\n",
    ")).dropna()\n",
    "\n",
    "ethereum_tweets.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaeb4e39-b9f2-4cf8-b401-c7505ccae043",
   "metadata": {},
   "outputs": [],
   "source": [
    "ethereum_tweets = ethereum_tweets[[\"date\", \"text\", \"hashtags\"]].set_index(\"date\")\n",
    "ethereum_tweets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe064ee-447d-4bcf-a59f-6fd31f1dc74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get rid of links and hashtags\n",
    "ethereum_tweets[\"text\"] = ethereum_tweets[\"text\"].apply(lambda x : ' '.join([s for s in x.split(' ') if s.find('@') == -1 and s.find('www') == -1 and s.find('https') == -1]))\n",
    "\n",
    "#get rid of non-ascii characters\n",
    "ethereum_tweets = ethereum_tweets.replace(r'\\W+', ' ', regex=True)\n",
    "\n",
    "\n",
    "# get rid of tweet duplicates to not bias the sentiment analysis\n",
    "ethereum_tweets = ethereum_tweets.drop_duplicates(subset=['text'])\n",
    "ethereum_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42ae64f-163d-4eff-af35-dadf71ccc456",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install textblob\n",
    "\n",
    "!pip install spacy\n",
    "!python -m textblob.download_corpora\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17911edb-4423-4a75-ad3e-5e4003092fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this code, we're using SpaCy for tokenization and part-of-speech tagging, and TextBlob for sentiment analysis. We define a custom analyze_sentiment function that takes a text input, processes it with SpaCy and TextBlob, and returns the sentiment label and polarity score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70c3264-f7a2-4486-b2b6-baa649f4a426",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from textblob import TextBlob\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def analyze_sentiment(text):\n",
    "    doc = nlp(text)\n",
    "    blob = TextBlob(text)\n",
    "    \n",
    "    polarity = blob.sentiment.polarity\n",
    "    \n",
    "    if polarity > 0:\n",
    "        sentiment = 'positive'\n",
    "    elif polarity < 0:\n",
    "        sentiment = 'negative'\n",
    "    else:\n",
    "        sentiment = 'neutral'\n",
    "    \n",
    "    return sentiment, polarity\n",
    "\n",
    "# Assuming df is your DataFrame with 'text' column\n",
    "ethereum_tweets['sentiment'], ethereum_tweets['polarity'] = zip(*ethereum_tweets['text'].apply(analyze_sentiment))\n",
    "\n",
    "df_sentiment = ethereum_tweets.sort_values('polarity').reset_index(drop=True)\n",
    "print(df_sentiment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b58abe-9e36-4fce-bcfe-94f0826b8a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sentiment[\"sentiment\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f3656b-6dd7-4ba1-8ebe-c9e58f55ab28",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_count = df_sentiment[df_sentiment['polarity'] > 0]['polarity'].count()\n",
    "negative_count = df_sentiment[df_sentiment['polarity'] < 0]['polarity'].count()\n",
    "neutral_count = df_sentiment[df_sentiment['polarity'] == 0]['polarity'].count()\n",
    "\n",
    "print(\"Positive count:\", positive_count)\n",
    "print(\"Negative count:\", negative_count)\n",
    "print(\"Neutral count:\", neutral_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2730b1be-42f2-464f-913d-c338193958bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before analyzing the content of the tweets, we are first going to preprocess our data even more. There are several preprocessing strategies we are going to:\n",
    "\n",
    "#Lemmatize each word\n",
    "#Delete extra characters\n",
    "#Remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93dc9bb-b42a-49aa-aa65-4cecb944351d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Load the English language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def preprocess(sentence, stemming=False, lemmatizing=False):\n",
    "    global counter\n",
    "    counter += 1\n",
    "    if counter % 100 == 0:\n",
    "        pass\n",
    "        # print(counter)\n",
    "\n",
    "    sentence = str(sentence)\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "    sentence = sentence.lower()\n",
    "    sentence = sentence.replace('{html}', \"\")\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, '', sentence)\n",
    "    rem_url = re.sub(r'http\\S+', '', cleantext)\n",
    "    rem_num = re.sub('[0-9]+', '', rem_url)\n",
    "    tokens = tokenizer.tokenize(rem_num)\n",
    "\n",
    "    filtered_words = [w for w in tokens if len(w) > 2 and w not in stopwords.words('english')]\n",
    "\n",
    "    if lemmatizing:\n",
    "        doc = nlp(\" \".join(filtered_words))\n",
    "        lemma_words = [token.lemma_ for token in doc if not token.is_punct and not token.is_space and not token.is_stop]\n",
    "        return \" \".join(lemma_words)\n",
    "\n",
    "    if stemming:\n",
    "        stemmer = PorterStemmer()\n",
    "        stem_words = [stemmer.stem(w) for w in filtered_words]\n",
    "        return \" \".join(stem_words)\n",
    "\n",
    "    return \" \".join(filtered_words)\n",
    "\n",
    "# Example usage\n",
    "df_sentiment['text'] = df_sentiment['text'].apply(lambda x: preprocess(x, stemming=False, lemmatizing=True))\n",
    "df_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c4445f-cb15-49e0-a15f-42f7318b953f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us extract the most common words found in both positive and negative positive reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9182b04-0781-462f-a978-4bd0b674e6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_neg = df_sentiment[df_sentiment['polarity'] < 0]\n",
    "df_pos = df_sentiment[df_sentiment['polarity'] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76daa03-b544-42ad-9d4e-1da4451817ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Amount of positive and negative reviews we have been inferring from our data,\n",
    "# Let's a have general idea about the opinion of the public regarding Ethereum tweets:\n",
    "print(\"Negative reviews\", len(df_neg))\n",
    "print(\"Positive reiews\", len(df_pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb127e19-ec48-495e-b6e8-2c037564ba65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Extracting the most common words found in both positive and negative positive reviews:\n",
    "positive_words = pd.DataFrame([dict(Counter(' '.join(df_pos['text'].values.tolist()).split(' ')))]).T.sort_values(0, ascending=False)[0:100].index\n",
    "\n",
    "negative_words = pd.DataFrame([dict(Counter(' '.join(df_neg['text'].values.tolist()).split(' ')))]).T.sort_values(0, ascending=False)[0:100].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b92deb-755a-4e23-8f6e-59023a81a327",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(\"Most common words in POSITIVE tweets on ETH:\",positive_words)\n",
    "display(\"Most common words in NEGATIVE tweets on ETH:\",negative_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1761d48-c28e-4dea-9347-5d89c98bc019",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
